---
title: "Final Assignment"
name: Liam Gilson
date: 6/9/2025
format: html
---

https://github.com/liamgilson18/ENVS-193DS_spring-2025_final

```{r, message=FALSE, warning=FALSE}
#read in code chunk

library(tidyverse) #load in tidyverse
library(dplyr)    # load in dplyr
library(lubridate)  #load in lubridate
library(forcats)  # load in forcats
library(tibble)   # load in tibble
library(DHARMa)
library(MuMIn)
library(ggeffects)
sst <- read.csv(here::here("data", "SST_update2023.csv")) #create object sst from data, problem 2
nested_boxes <- read.csv(here::here("data", "occdist.csv"))
```

# Problem 1

#### a. Transparent Statistical Methods

In part one, I conclude that they are doing a Pearsons correlation test, which is hinted at when it says, " We reject the null that there is no correlation..." meaning that there is evidence of correlation between head water distance and nitrogen load. I think its more likely to be a Pearsons correlation test because it makes logical sense that the closer we are to the headwater, the less nitrogen. So we would assume that there is a linear relationship, which there is. We also need the data to be continuous and numeric, which is true.

In part two, I conclude that they are doing a One Way Analysis of Variance test, which is hinted at when it says "...between sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands)" which means there are more than two groups, with one factor (nitrogen load).

#### b. More Information Needed

One additional test that should be included is a Tukey HSD test, which is a type of Post Hoc. Tukeys HSD tells us what groups are actually different from each other. For example the Tukey HSD test might tell us there is a significant difference between nitrogen from urban sources and atmospheric depositon, and not a significant difference between nitrogen from fertilizer and grasslands.

Another additional test what should be included is an effect size test (n^2^), which will explain how much the grouping variable explains the response. An effect size will tell us, for example, if most of the variance is coming from a specific source, such as fertilizer.

#### c. suggestions for rewriting

Part 1:

We found a \[(strong, medium, weak) (negative,positive) correlation between headwater (km) and annual total nitrogen load (kg year^-1^) using Pearsons correlation test (Pearsons r= correlation coefficient), such that the distance from headwaters significantly predicts the nitrogen load. (t= t-value, df= degrees of freedom, p=0.03, 95% confidence interval: \[lower bound, upper bound\], r=correlation coffiecient)

Part 2:

We reject the null hypothesis for the one way ANOVA test, (one-way ANOVA, F(among groups diff, within groups df) = f value, p=0.02, ⍺=significance level) and find that there is no difference in the average nitrogen load (kg year^-1^) . There was a (small, medium, large) difference (n^2^ = effect size) between sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands). After finding the source means with a 95% confidence interval, we were able to conducted a Tukeys HSD we found significant differences in these sources (source1-source2, difference in means between groups, (lower, upper bound 95% CI, adjusted p-value)

# Problem 2

#### a. Cleaning and Summarizing

```{r}

sst_clean <- sst |> #cleaning sst, creating object sst_clean
  mutate( #Convert date column and extract year and month
    date = as.Date(date),  # format for date
    year = factor(year(date)),  # extract year as an intiger
    month = factor(month(date, label = TRUE, abbr = TRUE), #read in month as "feb" or "Dec"
                   levels = month.abb, ordered = TRUE)  # use month.abb
  ) |>   #pipe command
  group_by(year, month) |> # Group by year and month, pipe command
  summarise(mean_monthly_sst = mean(temp, na.rm = TRUE), .groups = "drop") |> #clalculate mean temp and drop groups after. pipe command
  as_tibble()  # Step 4: Convert to tibble 

sst_clean |> slice_sample(n = 5) # Show 5 random rows

str(sst_clean) # Show structure of the tibble in
```

#### b.

```{r}


sst_filtered <- sst_clean |> # making object sst_filtered from sst_clean. pipe operator
  filter(as.numeric(as.character(year)) %in% 2018:2023)# Filter for years 2018–2023

blue_gradient <- c(
  "2018" = "#c6dbef",
  "2019" = "#9ecae1",
  "2020" = "#6baed6",#blue gradient.
  "2021" = "#4292c6",
  "2022" = "#2171b5",
  "2023" = "#084594"
)


ggplot(sst_filtered, aes(x = month, y = mean_monthly_sst, color = year, group = year)) + #skeleton of our plot made from sst_filtered. setting up axis labels and groups
  geom_point(size = 2) + #making point size 2
  geom_line(linewidth = 1) + #line width smaller than point so we can see the points
  labs( #labs let us title things
    x = "Month", #x axis title
    y = "Mean monthly sea surface temperature (°C)", # y axis title
    color = "Year" #legend title
  ) +
  theme_minimal(base_size = 14) +#set theme minimal and font size
  theme(
    panel.grid = element_blank(), # no grid
    axis.ticks = element_blank(), #no axis ticks
    axis.line = element_blank(), #no axis lines
    legend.position = c(0.05, 0.95), #legend position in the top left
    legend.justification = c("left", "top"), #legend position in the tiop left
    legend.title = element_text(size = 10), # font size for legend title
    legend.text = element_text(size = 8),#font size for legend text
    plot.title = element_text(size = 14, face = "bold"),#bold 
    plot.title.position = "plot", #irrelevant, we have no title
    legend.background = element_rect(fill = "white", color = NA),#make legend float
    panel.border = element_rect(color = "black", fill = NA, linewidth = 0.5),#panel color and width
    plot.margin = margin(10, 10, 10, 10)#set margins
  ) +
  scale_color_manual(values = blue_gradient) +  # gradient
  coord_cartesian(clip = "off")
```

# Problem 3

#### a. Response Variable

The 1's and 0's are a binary data, where 1 means that the specific bird did occupy the box more than once, and 0 if it did not. For each data entry, the accounted for birds are Swift Parrot, Common Starling, and Tree Martin.

#### b. Purpose of Study

The difference is that the Swift Parrot is a target species and the Common Sterling and Tree Martin are non-target species. The nested boxes are made to help the endangered Swift Parrot breed, without inviting rival species such as the Common Sterling and Tree Martin.

#### c. Difference in "Seasons"

The authors are talking about comparing breeding seasons between 2016 and 2019, where the breeding season is summertime, during mast flowering events. In 2016 these boxes are brand new, and by 2019 they are 3 years old and are established nested boxes.

#### d. Table of Models

| Model number | Season | Distance from Forest Edge | Predictor list |
|----|----|----|----|
| 1 |  |  | null model (no predictors) |
| 2 | x | x | saturated model (season & distance) |
| 3 |  | x | Distance only model |
| 4 | x |  | Season only model |

#### e. Run the Models

```{r}
# Clean data: create binary occupancy variable
nested_boxes$occupied <- ifelse(nested_boxes$box.occupant == "empty" | is.na(nested_boxes$box.occupant), 0, 1)

# Convert 'season' to a factor to treat it as categorical (year categories)
nested_boxes$season <- as.factor(nested_boxes$season)

# Fit models with 'season' as a factor
mod1 <- glm(occupied ~ 1, family = binomial, data = nested_boxes)                   # null model
mod2 <- glm(occupied ~ season * edge.distance, family = binomial, data = nested_boxes)  # saturated model (interaction)
mod3 <- glm(occupied ~ edge.distance, family = binomial, data = nested_boxes)         # distance only model
mod4 <- glm(occupied ~ season, family = binomial, data = nested_boxes)                # season only model

```

#### f. Check the Diagnostics

```{r}
plot(simulateResiduals(mod1), main = "Model 1: Null model")#diagnosic test for mod1
plot(simulateResiduals(mod2), main = "Model 2: Saturated model")#diagnosic test for mod2
plot(simulateResiduals(mod3), main = "Model 3: Distance to edge only") #diagnosic test for mod3
plot(simulateResiduals(mod4), main = "Model 4: Season only model") #diagnosic test for mod4
```

#### g. Select the Best Model

```{r}
model_selection <- AICc(mod1, mod2, mod3, mod4) |> # Compare AIC values for all models
arrange(AICc) #arrange them in descending order
model_selection#View in terminal
```

The best model as determined by Akaike’s Information Criterion (AIC) is the model including both distance and mating season.

#### h. Visualizing the Model Predictions

```{r}




```

# Problem 4

#### a. Comparing Visualizations

My visualizations are different from each other significantly. Whereas my homework 2 visualization were pure data, in the form of a histogram and box plot, my homework 3 visualization had an artistic component, where I displayed each week of the quarter as a different slice of pizza. This way, its more pleasing visually, and you can see that each week i took a bigger or smaller 'slice' out of my savings account when I eat out.

There definitely are similarities, In my affective visualization, I still used a version of a pie chart, which I see as a simple graph like the histogram and box plot. In this sense, what I was displaying was very simple, and modeled after elementary graphs.

In my first visualization, I can see that when I eat out, I tend to spend much more money when I go to Deja Vu. In my second visualization, I can see a trend in my average spending for the "snack category, which is skewed left, with the median pushed to the left of the entire range, hinting that when I get snacks, I sometimes spend a lot, but usually don't. In my third visualization I can see that the general trend over all the weeks of the spring quarter, that I did in fact spend more money in the later weeks of the quarter.

My feedback from week 9 was to make the separation of the pieces of pizza cleaner, because originally it was just lines generated in google drawings, and my second was to make the slices of pie accurate to my data, which it originally was not. I implemented the second change, for which i made a pie chart of my data, and then projected those lines onto my picture of pizza.
