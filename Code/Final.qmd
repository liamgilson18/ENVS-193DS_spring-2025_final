---
title: "Final Assignment"
name: Liam Gilson
date: 6/9/2025
format: html
---

https://github.com/liamgilson18/ENVS-193DS_spring-2025_final

```{r, message=FALSE, warning=FALSE}
#read in code chunk

library(tidyverse) #load in tidyverse
library(dplyr)    # load in dplyr
library(lubridate)  #load in lubridate
library(forcats)  # load in forcats
library(tibble)   # load in tibble
sst <- read.csv(here::here("data", "SST_update2023.csv")) #create object sst from data downloaded
```

# Problem 1

#### a. Transparent Statistical Methods

In part one, I conclude that they are doing a Pearsons correlation test, which is hinted at when it says, " We reject the null that there is no correlation..." meaning that there is evidence of correlation between head water distance and nitrogen load. I think its more likely to be a Pearsons correlation test because it makes logical sense that the closer we are to the headwater, the less nitrogen. So we would assume that there is a linear relationship, which there is. We also need the data to be continuous and numeric, which is true.

In part two, I conclude that they are doing a One Way Analysis of Variance test, which is hinted at when it says "...between sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands)" which means there are more than two groups, with one factor (nitrogen load).

#### b. More Information Needed

One additional test that should be included is a Tukey HSD test, which is a type of Post Hoc. Tukeys HSD tells us what groups are actually different from each other. For example the Tukey HSD test might tell us there is a significant difference between nitrogen from urban sources and atmospheric depositon, and not a significant difference between nitrogen from fertilizer and grasslands.

Another additional test what should be included is an effect size test (n^2^), which will explain how much the grouping variable explains the response. An effect size will tell us, for example, if most of the variance is coming from a specific source, such as fertilizer.

#### c. suggestions for rewriting

Part 1:

We found a \[(strong, medium, weak) (negative,positive) correlation between headwater (km) and annual total nitrogen load (kg year^-1^) using Pearsons correlation test (Pearsons r= correlation coefficient), such that the distance from headwaters significantly predicts the nitrogen load. (t= t-value, df= degrees of freedom, p=0.03, 95% confidence interval: \[lower bound, upper bound\], r=correlation coffiecient)

Part 2:

We reject the null hypothesis for the one way ANOVA test, (one-way ANOVA, F(among groups diff, within groups df) = f value, p=0.02, ⍺=significance level) and find that there is no difference in the average nitrogen load (kg year^-1^) . There was a (small, medium, large) difference (n^2^ = effect size) between sources (urban land, atmospheric deposition, fertilizer, wastewater treatment, and grasslands). After finding the source means with a 95% confidence interval, we were able to conducted a Tukeys HSD we found significant differences in these sources (source1-source2, difference in means between groups, (lower, upper bound 95% CI, adjusted p-value)

# Problem 2

#### a. Cleaning and Summarizing

```{r}

sst_clean <- sst |> #cleaning sst, creating object sst_clean
  mutate( #Convert date column and extract year and month
    date = as.Date(date),  # format for date
    year = factor(year(date)),  # extract year as an intiger
    month = factor(month(date, label = TRUE, abbr = TRUE), #read in month as "feb" or "Dec"
                   levels = month.abb, ordered = TRUE)  # use month.abb
  ) |>   #pipe command
  group_by(year, month) |> # Group by year and month, pipe command
  summarise(mean_monthly_sst = mean(temp, na.rm = TRUE), .groups = "drop") |> #clalculate mean temp and drop groups after. pipe command
  as_tibble()  # Step 4: Convert to tibble 

sst_clean |> slice_sample(n = 5) # Show 5 random rows

str(sst_clean) # Show structure of the tibble in
```

#### b.  

```{r}

# Filter for years 2018–2023
sst_filtered <- sst_clean |> 
  filter(as.numeric(as.character(year)) %in% 2018:2023)

blue_gradient <- c(
  "2018" = "#c6dbef",
  "2019" = "#9ecae1",
  "2020" = "#6baed6",
  "2021" = "#4292c6",
  "2022" = "#2171b5",
  "2023" = "#084594"
)

# Plot
ggplot(sst_filtered, aes(x = month, y = mean_monthly_sst, color = year, group = year)) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  labs(
    x = "Month",
    y = "Mean monthly sea surface temperature (°C)",
    color = "Year"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid = element_blank(),
    axis.ticks = element_blank(),
    axis.line = element_blank(),
    legend.position = c(0.05, 0.95),
    legend.justification = c("left", "top"),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 8),
    plot.title = element_text(size = 14, face = "bold"),
    plot.title.position = "plot",
    legend.background = element_rect(fill = "white", color = NA),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 0.5),
    plot.margin = margin(10, 10, 10, 10)
  ) +
  scale_color_manual(values = blue_gradient) +  # <- This must stay in the chain
  coord_cartesian(clip = "off")
 
#xtill need to annotate every code line!!!!!
```
